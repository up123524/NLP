#!/usr/bin/env python3
# -*-encoding utf-8 -*-
"""
------------------------------------------------------------------------------
    ai_in_finance.py    |    LDA on an economic newspaper corpus
------------------------------------------------------------------------------

Author   : Umar Patel

Synopsis : The corpus oftext contains documents published over circa 10 years.
           This LDA model analyzes the documents as a pooled cross section model
           and estimates the number of topics that best fit the data.
Notes    : the code has been tested with Gensim 3.8.3, spaCy XX, and 
           Tomotpy xx.xx

"""

# %%
# load libraries
import os
from tracemalloc import stop
from langcodes import best_match
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rc
import pandas as pd
import spacy
import tomotopy as tp
from gensim.models import Phrases

# %%
# plot style
plt.style.use("Solarize_Light2")

# %%
# working dir
os.chdir("../sampleData/econNewspaper")

# %%
# load data

# load corpus
df = pd.read_csv("ft_wsj.csv")

# %%
# clean the data

# basic cleaning
# --+ date as date time
df.loc[:, "date"] = pd.to_datetime(df.loc[:, "date"])
# --+ get timespans
df.loc[:, "year"] = df["date"].dt.year
# --+ slice the data
"""
let's focus on the 2013 - 2019 timespan, which concentrates the large majority
of the data.
"""
df = df.loc[df["year"] >= 2013]
# --+ drop column
df.drop(["_id"], axis=1, inplace=True)
# --+ remove line breaks and unicode stuff
to_remove = ["\n", "\x19s", "\x1c"]
for item in to_remove:
    df.loc[:, "text"] = df.loc[:, "text"].str.replace(item, "")

# %%
# pre-process the data w/spaCy

# the spaCy pipeline
nlp = spacy.load("en_core_web_lg")
# expand the list of stopwords
stopwords = ["Mr", "Mr.", "$", "Inc.", "year"]
for item in stopwords:
    nlp.vocab[item].is_stop = True
# empty containers
docs_tokens = []
# iterate over the documents
for doc in nlp.pipe(
    df.loc[:, "text"].to_list(),
    batch_size=1000,
    disable=["tok2vec", "tagger", "parser", "attribute_ruler"],
):
    docs_tokens.append(
        [
            token.lemma_
            for token in doc
            if not token.is_stop and not token.is_punct and not token.like_num
        ]
    )

# %%
# find bigrams and trigrams

# get rid of common terms
common_terms = [
    u"of",
    u"with",
    u"without",
    u"and",
    u"or",
    u"the",
    u"a",
    u"not",
    "be",
    u"to",
    u"this",
    u"who",
    u"in",
]
# find phrases
bigram = Phrases(
    docs_tokens,
    min_count=50,
    threshold=5,
    max_vocab_size=50000,
    common_terms=common_terms,
)
trigram = Phrases(
    bigram[docs_tokens],
    min_count=50,
    threshold=5,
    max_vocab_size=50000,
    common_terms=common_terms,
)
# uncomment if bi-grammed, tokenized document is preferred
# docs_phrased = [bigram[line] for line in docs_tokens]
docs_phrased = [trigram[bigram[line]] for line in docs_tokens]

# %%
# create a corpus using Tomotopy utils

# empty corpus
corpus = tp.utils.Corpus()
# populate the corpus
for item in docs_phrased:
    corpus.add_doc(words=item)

# %%
# topic modeling â€• explore model validity

# register "UMass" coherence scores
cvs = {}
for topic_number in range(1, 16, 1):
    mdl = tp.LDAModel(k=topic_number, corpus=corpus)
    for i in range(0, 100, 10):
        mdl.train(10)
        print("Iteration: {}\tLog-likelihood: {}".format(i, mdl.ll_per_word))
    coh = tp.coherence.Coherence(mdl, coherence="u_mass")
    cvs[topic_number] = coh.get_score()
# plot coherence scores
fig = plt.figure(figsize=(6, 4))
ax = fig.add_subplot(111)
ax.plot(cvs.keys(), cvs.values(), "o-")
ax.set_xlabel("Number of topics retained")
ax.set_ylabel("Coherence score")
ax.set_xticks(range(1, 16, 1))
plt.show()

# %%
# topic model estimation

# let's go with the six topic model
best_mdl = tp.LDAModel(k=11, corpus=corpus)
for i in range(0, 100, 10):
    best_mdl.train(10)
    print("Iteration: {}\tLog-likelihood: {}".format(i, best_mdl.ll_per_word))

# %%
# word to topic probabilities

# an empty Pandas DF to populate
wt = pd.DataFrame()
# get word probabilities for each topic
for k in range(best_mdl.k):
    words, probs = [], []
    for word, prob in best_mdl.get_topic_words(k):
        words.append(word)
        probs.append(prob)
    tmp = pd.DataFrame(
        {
            "word": words,
            "prob": np.round(probs, 3),
            "k": np.repeat(k, len(words)),
            "sort": np.arange(0, len(words)),
        }
    )
    wt = pd.concat([wt, tmp], ignore_index=False)
    del tmp

# %%
# doc to topic probabilities

# get topic probabilities for each document
td = pd.DataFrame(
    np.stack([doc.get_topic_dist() for doc in best_mdl.docs]),
    columns=["topic_{}".format(i + 1) for i in range(best_mdl.k)],
)


